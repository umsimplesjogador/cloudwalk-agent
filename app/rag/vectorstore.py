import os
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from app.llm.gemini_client import ask_gemini

DATA_DIR = "data"
MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

class VectorStore:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name=MODEL_NAME)
        self.index_path = os.path.join(DATA_DIR, "faiss_index")

        # Reconstr√≥i √≠ndice se n√£o existir
        if not os.path.exists(self.index_path):
            from app.rag.ingest import RAGIngest
            print("‚öôÔ∏è √çndice FAISS n√£o encontrado. Construindo com RAGIngest...")
            RAGIngest().build_index()

        print("üìÇ Carregando √≠ndice FAISS existente...")
        self.vectorstore = FAISS.load_local(self.index_path, self.embeddings, allow_dangerous_deserialization=True)

        # Prompt refor√ßado para garantir todos os pilares
        template = (
            "Voc√™ √© um assistente especializado em responder perguntas sobre a empresa Cloudwalk e o site InfinitePay.\n"
            "Baseie suas respostas **exclusivamente** no contexto fornecido abaixo.\n"
            "‚ö†Ô∏è Somente mencione os pilares (Best Product, Customer Engagement, Disruptive Economics) se a pergunta for **explicitamente sobre pilares, valores ou vis√£o**.\n"
            "Se a pergunta **n√£o mencionar** esses temas, **n√£o inclua os pilares** em hip√≥tese alguma.\n"
            "Se a pergunta for sobre filosofia ou religi√£o, responda **exclusivamente** com FIAT LUX e sua descri√ß√£o.\n"
            "Se a pergunta for sobre miss√£o, utilize **todo o conte√∫do textual** encontrado no link https://www.cloudwalk.io/#our-mission, incluindo a explica√ß√£o completa sobre a democratiza√ß√£o da ind√∫stria financeira, a import√¢ncia do pre√ßo e o objetivo de construir um novo sistema justo para todos.\n"
            "Mantenha o texto em portugu√™s natural e fluido, traduzindo integralmente o conte√∫do original em ingl√™s.\n"
            "Quando citar os pilares, mantenha os nomes em ingl√™s, mas **traduza as descri√ß√µes para o portugu√™s**, preservando o sentido original.\n"
            "Responda de forma organizada, com formata√ß√£o em t√≥picos e linguagem natural.\n\n"
            "Contexto:\n{context}\n\n"
            "Pergunta: {question}\n\n"
            "Resposta:"
        )
        
        self.prompt = PromptTemplate(template=template, input_variables=["context", "question"])

    def query(self, question: str, top_k: int = 10):
        try:
            # Recupera os documentos mais relevantes
            docs = self.vectorstore.similarity_search(question, k=top_k)

            # Garante que se a pergunta for sobre pilares, tragam TODOS da se√ß√£o "our-pillars"
            if "pillar" in question.lower() or "pilares" in question.lower():
                all_docs = self.vectorstore.similarity_search("pillars", k=50)
                docs = [d for d in all_docs if "pillar" in d.metadata.get("section", "").lower()]

            # Monta o contexto completo (sem cortar)
            context_blocks = []
            for d in docs:
                section = d.metadata.get("section", "desconhecida")
                text = d.page_content
                context_blocks.append(f"[Se√ß√£o: {section}] {text}")

            context = "\n\n".join(context_blocks)

            # Monta e envia prompt ao Gemini
            prompt_text = self.prompt.format(context=context, question=question)
            answer = ask_gemini(prompt_text)

            sources = list({d.metadata.get("source", "desconhecida") for d in docs})

            return {"hits": [{"text": d.page_content, "meta": d.metadata} for d in docs],
                    "answer": answer,
                    "sources": sources}

        except Exception as e:
            print("‚ùå Erro ao consultar o modelo:", e)
            return {"hits": [], "answer": "Erro ao processar a pergunta.", "sources": []}
